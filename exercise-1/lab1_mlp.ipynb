{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise I:<br> Multi-layer perceptrons\n",
    "for classification and regression problems.\n",
    "</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* train MLPs and for both classification and regression problems\n",
    "* overtraining models and see how overtraining changes validation performance\n",
    "* avoid overtraining using regularization\n",
    "* do model selection\n",
    "\n",
    "There are 10 questions in this exercise. These 10 question can be found in four different cells below (also see section \"The Different Cells\"). All of the exercises deal with training and evaluation of the multi-layer perceptron (MLP) network. You are going to work with different datasets (see below), binary classification problems and function approximation (regression) problems. For all questions, except the last one, code is available that you can run directly or only need to make small modifications to. For the last question we only provide a small part of the code and you should provide the rest. However, it is typically just a matter of paste and copy from the previous code cells (in a proper way).\n",
    "\n",
    "You will write the report of the exercise within this notebook. The details of how to do that can be found below in section \"Writing the report\". Finally before you start:\n",
    "\n",
    "**Deadline for submitting the report: December 7, 12:00 (2017)**\n",
    "\n",
    "\n",
    "## The data\n",
    "There are several datasets in this exercise. \n",
    "\n",
    "### syn1 - syn3\n",
    "Three different synthetic classification problems will be used. They are all 2D binary classification problems which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. They are called *syn1, syn2* and *syn3*. Each of these datasets are generated \"on the fly\" each time. They come from various normal distributions. Since they are generated using random numbers it means that each time you generate the data it will be slightly different from next time. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot these datasets.\n",
    "\n",
    "### regr1\n",
    "There is also a synthetic regression problem, called *regr1*. It has 6 inputs (independent variables) and one output variable (dependent variable). It is generated according to the following formula:  \n",
    "\n",
    "$\\qquad d = 2x_1 + x_2x_3^2 + e^{x_4} + 5x_5x_6 + 3\\sin(2\\pi x_6) + \\alpha\\epsilon$  \n",
    "    \n",
    "where $\\epsilon$ is added normally distributed noise and $\\alpha$ is a parameter controlling the size of the added noise. Variables $x_1,...,x_4$ are normally distrubuted with zero mean and unit variance, whereas $x_5, x_6$ are uniformly distributed ($[0,1]$). The target value $d$ has a non-linear dependence on ***x***.\n",
    "\n",
    "### Pima Indians diabetes dataset\n",
    "*This data set is taken from the UCI Machine Learning Repository [http://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes].* In short this dataset contains 8 independent variables (clinical data) and a binary target value, where 1 indicates \"tested positive for diabetes\". In total there are 768 cases in the dataset, divided into a fixed training dataset of 538 cases and a validation dataset of the remaining 230 cases. The dataset is provided as the two files *pima-trn.csv* and *pima_val.csv*. \n",
    "\n",
    "## The exercises\n",
    "There are 10 questions, where the first 5 questions are dealing with 2D binary classification problems. Here you will be able to see the boundary implemented by the different MLPs that you train. Questions 6-9 deals with training a regression network for the *regr1* dataset. Here you are also going to use regularization to \"combat\" overtraining. Finally the last question, here your task is to come up with a model for the Pima classification problem. You should make a model that optimizes the validation result.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | MLP | Needed | Defines the MLP model |\n",
    "| 3 | Data | Needed | Defines the functions to generate the artificial datasets |\n",
    "| 4 | PlotData | Information | Plots the 2D classification datasets |\n",
    "| 5 | Statistics | Needed | Defines the functions that calculates various performance measures |\n",
    "| 6 | Boundary | Needed | Function that can show 2D classification boundaries |\n",
    "| 7 | Ex1 | Exercise | For question 1 |\n",
    "| 8 | Ex2 | Exercise | For question 2-5 |\n",
    "| 9 | Ex3 | Exercise | For question 6-9 |\n",
    "| 10 | Ex4 | Exercise | For question 10 |\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells with the celltype \"Needed\". The very first time you start with this exercise we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work you way down the cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "First the report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a small introduction of what you have done in the lab.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or a longer ones depending on the nature of the questions, but try to be effective in your writing.\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "It is important that you write the report in this last cell and **not** after each question! Also when uploading your report to Live@Lund, name the file according to:\n",
    "\n",
    "**lab1_Surname_Firstname.ipynb**\n",
    "\n",
    "## Last but not least\n",
    "Have fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Init (#1)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Initializing the libraries\n",
    "\n",
    "In the cell below, we import all the libraries that are needed for this exercises. There are two configuration parameters that you can change in this cell\n",
    "\n",
    "* The size of the plots\n",
    "* Inline or \"pop out\" plots.\n",
    "\n",
    "See comments in the cell for more information. Default is inline plots with a \"lagom\" size. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import metrics, regularizers\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# The size of the plots.\n",
    "mpl.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "\n",
    "# To have the plots inside the notebook \"inlin\" should be True. \n",
    "# If \"inlin\" = False, then plots will pop out of the notebook\n",
    "inlin = True # True/False\n",
    "if inlin:\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: MLP (#2)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n",
    "\n",
    "* inp_dim: the input dimension (integer)\n",
    "\n",
    "* n_nod: size of the network, eg [5] for a one hidden layer with 5 nodes and [5,3] for a two layer network with 5 and 3 hidden nodes each.\n",
    "\n",
    "* act_fun: the activation function. Most common are\n",
    "    * 'linear'\n",
    "    * 'relu'\n",
    "    * 'tanh'\n",
    "    * 'sigmoid'\n",
    "        \n",
    "* out_act_fun: the activation function for the output nodes. Most common are\n",
    "    * 'linear'\n",
    "    * 'sigmoid'\n",
    "    * 'softmax'\n",
    "    \n",
    "* opt_method: The error minimization method. Common choices\n",
    "    * 'SGD'\n",
    "    * 'Adam'\n",
    "    * 'Nadam'\n",
    "    * 'RMSprop'\n",
    "    \n",
    "* cost_fun: The error function used during training. There are three common ones\n",
    "    * 'mean_squared_error'\n",
    "    * 'binary_crossentropy'\n",
    "    * 'crossentropy?'\n",
    "\n",
    "* lr_rate: The learning rate. Note some of the minimization methods uses a dynamical learning rate. In such a cases this value sets the initial learning rate.\n",
    "\n",
    "* lambd: L2 regularization parameter\n",
    "\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipline(inp_dim,\n",
    "            n_nod,\n",
    "            act_fun = 'relu',\n",
    "            out_act_fun = 'sigmoid',\n",
    "            opt_method = 'Adam',\n",
    "            cost_fun = 'binary_crossentropy',\n",
    "            lr_rate = 0.01, \n",
    "            lambd = 0.0):\n",
    "    \n",
    "    lays = [inp_dim] + n_nod\n",
    "    \n",
    "    main_input = Input(shape=(inp_dim,), dtype='float32', name='main_input')\n",
    "    \n",
    "    X = main_input\n",
    "    for nod in n_nod:\n",
    "        X = Dense(nod, \n",
    "                  activation = act_fun,\n",
    "                  kernel_regularizer=regularizers.l2(lambd))(X)\n",
    "        \n",
    "    output = Dense(1, activation = out_act_fun )(X)\n",
    "    \n",
    "    method = getattr(keras.optimizers, opt_method)\n",
    "    \n",
    "    model =  Model(inputs=[main_input], outputs=[output])\n",
    "    model.compile(optimizer = method(lr = lr_rate),\n",
    "                  loss = cost_fun)   \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Data (#3)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining synthetic data sets\n",
    "\n",
    "This cell defines the three different synthetic data sets and the regression dataset. Run the cell by entering into the\n",
    "cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def syn1(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "    \n",
    "    global seed\n",
    "    \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "  \n",
    "    data[:N1,0] = 4 + np.random.normal(loc=.0, scale=1., size=(N1))\n",
    "    data[N1:,0] = -4 + np.random.normal(loc=.0, scale=1., size=(N-N1))\n",
    "    data[:,1] = 10*np.random.normal(loc=.0, scale=1., size=(N))\n",
    "    \n",
    "    \n",
    "    data = data / data.std(axis=0)\n",
    "    \n",
    "    # Target\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "    \n",
    "    # Rotation\n",
    "    theta = np.radians(30)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array([[c,-s],[s,c]]) # rotation matrix\n",
    "    data = np.dot(data,R) \n",
    "    \n",
    "    return data,tar\n",
    "\n",
    "\n",
    "def syn2(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "    \n",
    "    global seed\n",
    "     \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "\n",
    "    # Positive samples\n",
    "    data[:N1,:] = 0.8 + np.random.normal(loc=.0, scale=1., size=(N1,2))\n",
    "    # Negative samples \n",
    "    data[N1:,:] = -.8 + np.random.normal(loc=.0, scale=1., size=(N-N1,2))\n",
    "    \n",
    "    \n",
    "    # Target\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "\n",
    "    return data,tar\n",
    "\n",
    "\n",
    "def syn3(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "\n",
    "    global seed\n",
    "    \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(2*N/3)\n",
    "    \n",
    "    # disk\n",
    "    teta_d = np.random.uniform(0, 2*np.pi, N1)\n",
    "    inner, outer = 2, 5\n",
    "    r2 = np.sqrt(np.random.uniform(inner**2, outer**2, N1))\n",
    "    data[:N1,0],data[:N1,1] = r2*np.cos(teta_d), r2*np.sin(teta_d)\n",
    "        \n",
    "    #circle\n",
    "    teta_c = np.random.uniform(0, 2*np.pi, N-N1)\n",
    "    inner, outer = 0, 3\n",
    "    r2 = np.sqrt(np.random.uniform(inner**2, outer**2, N-N1))\n",
    "    data[N1:,0],data[N1:,1] = r2*np.cos(teta_c), r2*np.sin(teta_c)\n",
    "    \n",
    "    # Normalization\n",
    "    #data = data - data.mean(axis=0)/data.std(axis=0)\n",
    "\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "    \n",
    "    return data, tar\n",
    "\n",
    "\n",
    "def regr1(N, v=0):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "\n",
    "    global seed\n",
    "\n",
    "    data = np.empty(shape=(N,6), dtype = np.float32)  \n",
    "    \n",
    "    uni = lambda n : np.random.uniform(0,1,n)\n",
    "    norm = lambda n : np.random.normal(0,1,n)\n",
    "    noise =  lambda  n : np.random.normal(0,1,n)\n",
    "    \n",
    "    \n",
    "    for i in range(4):\n",
    "        data[:,i] = norm(N)\n",
    "    for j in [4,5]:\n",
    "        data[:,j] = uni(N)\n",
    "    \n",
    "    tar =   2*data[:,0] + data[:,1]* data[:,2]**2 + np.exp(data[:,3]) + \\\n",
    "            5*data[:,4]*data[:,5]  + 3*np.sin(2*np.pi*data[:,5])\n",
    "    std_signal = np.std(tar)\n",
    "    tar = tar + v * std_signal * noise(N)\n",
    "        \n",
    "    return data, tar\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: PlotData (#4)\n",
    "### CellType: Information\n",
    "### Cell instruction: Plotting the data\n",
    "\n",
    "Here we just generate 100 cases for each data set and plot them. Run the cell by entering into the\n",
    "cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the three different 2D synthetic classification data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "d,t = syn1(100)\n",
    "plt.figure(1)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = syn2(100)\n",
    "plt.figure(2)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = syn3(100)\n",
    "plt.figure(3)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Statistics (#5)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Present result for both classification and regression problems\n",
    "\n",
    "This cell defines two functions that we are going to call using a trained model to calculate both error and performance measures. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats_class(x = None, y = None, label = 'Training', modl = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             y = output\n",
    "             label = \"Provided text string\"\n",
    "             modl = the model\n",
    "             \n",
    "    output : \n",
    "             sensitivity = fraction of correctly classified positive cases\n",
    "             specificity = fraction of correctly classified negative cases\n",
    "             accuracy = fraction of correctly classified cases\n",
    "             loss = typically the cross-entropy error\n",
    "    \"\"\"\n",
    "    \n",
    "    def binary(y1):\n",
    "        y1[y1>.5] = 1.\n",
    "        y1[y1<= .5] = 0.        \n",
    "        return y1\n",
    "\n",
    "    y_pr = modl.predict(x, batch_size = x.shape[0], verbose=0).reshape(y.shape)\n",
    "                \n",
    "    nof_p, tp, nof_n, tn = [np.count_nonzero(k) for k in [y==1, y_pr[y==1.] > 0.5, y==0, y_pr[y==0.]<= 0.5]]\n",
    "    \n",
    "    sens = tp / nof_p\n",
    "    spec = tn / nof_n\n",
    "    acc = (tp + tn) / (len(y))\n",
    "    loss = modl.evaluate(x, y , batch_size =  x.shape[0], verbose=0)\n",
    "                \n",
    "    A = ['Accuracy', 'Sensitivity', 'Specificity', 'Loss']\n",
    "    B = [acc, sens, spec, loss]\n",
    "    \n",
    "    print('\\n#############  STATISTICS for {} Data ##############\\n'.format(label))\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n#########################################################\\n')  \n",
    "\n",
    "\n",
    "def stats_reg(d = None, d_pred = None, label = 'Training', estimat = None):\n",
    "    \n",
    "    A = ['MSE', 'CorrCoeff']\n",
    "    \n",
    "    pcorr = np.corrcoef(d, d_pred)[1,0]\n",
    "    \n",
    "    if label.lower() in ['training', 'trn', 'train']:\n",
    "        mse = estimat.history['loss'][-1]\n",
    "    else:\n",
    "        mse = estimat.history['val_loss'][-1] \n",
    "\n",
    "    B = [mse, pcorr]\n",
    "    \n",
    "    print('\\n#############  STATISTICS for {} Data ##############\\n'.format(label))\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n###########################################################\\n')  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Boundary (#6)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Decision boundary\n",
    "\n",
    "This cell defines the function to plot the decision boundary for a 2D input binary MLP classifier. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_b(X = None, Y1 = None ):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    Z[Z>.5] = 1\n",
    "    Z[Z<= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X, batch_size = X.shape[0], verbose=0).reshape(Y1.shape)\n",
    "  \n",
    "    Y = np.copy(Y1)\n",
    "    Y_pr[Y_pr>.5] = 1\n",
    "    Y_pr[Y_pr<= .5] = 0\n",
    "    Y[(Y!=Y_pr) & (Y==0)] = 2\n",
    "    Y[(Y!=Y_pr) & (Y==1)] = 3\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9) \n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    \n",
    "    plt.scatter(X[:, 0][Y==1], X[:, 1][Y==1], marker='+', c='k')\n",
    "    plt.scatter(X[:, 0][Y==0], X[:, 1][Y==0], marker='o', c='k')\n",
    "       \n",
    "    plt.scatter(X[:, 0][Y==3], X[:, 1][Y==3], marker = '+', c='r')   \n",
    "    plt.scatter(X[:, 0][Y==2], X[:, 1][Y==2], marker = 'o', c='r')\n",
    "    \n",
    "    \n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex1 (#7)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 1\n",
    "\n",
    "The cell below should be used for question 1. You can run the cell as it is (i.e. CTRL-Return). However, looking at the code will help you understand how the network is created, trained and evaluated. It will be useful for the other questions.\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Use synthetic data 1 (syn1) (100 data points) and train a linear MLP to separate the two classes, i.e. use a single hidden node. **Why can you solve this problem with a single hidden node?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_train, d_train = syn1(100)\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'sgd',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.1                    # learningrate\n",
    "        }\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 300,                     # Number of epochs\n",
    "                      #validation_data=(x_val, y_val),  # We don't have any validation dataset!\n",
    "                      batch_size = x_train.shape[0],    # Use batch learning\n",
    "                      #batch_size=25,                   \n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for the training\n",
    "stats_class(x_train, d_train, 'Training', model)\n",
    "\n",
    "# Some plotting\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary\n",
    "decision_b(x_train, d_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex2 (#8)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 2-5\n",
    "\n",
    "The cell below should be used for questions 2-5. For question 2 you can run the cell as it is (i.e. CTRL-Return). For the other questions you need to modify the cell in order to change data set, vary the size of the network etc. There are brief comments in the code that will guide you here.\n",
    "\n",
    "From now on we will talk about *performance*! It can be performance of a trained model on the training dataset or the performance on the validation dataset. What do we mean by performance?  For classification problems we will provide 4 different measurements as returned by a call to the *stats_class* function. They are:\n",
    "* Sensitivity = fraction of correctly classified \"1\" cases\n",
    "* Specificity = fraction of correctly classified \"0\" cases\n",
    "* Accuracy = fraction of correctly classified cases\n",
    "* loss = cross-entropy error\n",
    "\n",
    "Our suggestion for you is to either use accuracy or loss as your performance measure.\n",
    "\n",
    "#### Question 2\n",
    "Here you are going to train a classifier for the *syn2* dataset. You are also going to use a validation dataset as an estimate of the *true* performance. Since we generate these datasets we can allow for a relatively large validation dataset in order to get a more accurate estimation of *true* performance. The default value in the cell is to generate 1000 validation datapoints. \n",
    "\n",
    "Now, use synthetic data 2 (syn2)(100 training data points) and train a *linear* MLP to separate the two classes, i.e. use a single hidden node. **What is the performance you get on the validation dataset?** Note: Use a fixed random seed for this exercise since you will compare with runs in the next question.\n",
    "\n",
    "#### Question 3\n",
    "You are now going to overtrain the MLP! By increasing the number of hidden nodes we should be able to get better and better training performance. **How many hidden nodes do you need to reach an accuracy >95% on your training dataset?**\n",
    "\n",
    "**Hint:** Overtraining here often means finding good local minimum of the error function, which may require some tuning of the learning parameters. This means that you may have to change the learning rate, increase the number of epochs and use \"better\" minimization methods. Even though we have not yet talked about the *Adam* minimization method, it is generally better than vanilla *stochastic gradient descent*. It is therefore used in the cells below as the default minimizer. Also you may want to change the size of the \"batch_size\" parameter. It is by default using all data.\n",
    "\n",
    "#### Question 4\n",
    "The effect of overtraining can be monitored by looking at the validation performance. **(a) When you overtrained in the previous question, how much much did the validation *loss* increase, compared to the linear model of Q2?** **(b) What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?** \n",
    "\n",
    "#### Question 5\n",
    "Now you are going to use the *syn3* dataset. So, use **150** training datapoints from the synthetic dataset 3 and train an MLP to separate the two classes. Also use about 1000 datapoints for validation. By looking at the data, one would expect to need at least 3 hidden neurons to separate the two classes. **Why?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 3\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "x_train, d_train = syn2(100)\n",
    "x_val, d_val = syn2(1000)\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'adam',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.07                     # learningrate\n",
    "         }\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 1000,      \n",
    "                      validation_data=(x_val, d_val),\n",
    "                      batch_size = x_train.shape[0],    # Batch size = all data (batch learning)\n",
    "                      #batch_size=50,                   # Batch size for true SGD\n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "stats_class(x_train, d_train, 'Training', model)\n",
    "stats_class(x_val, d_val, 'Validation', model)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.plot(estimator.history['val_loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary for the training dataset\n",
    "decision_b(x_train, d_train)\n",
    "\n",
    "# If you uncomment this one you will see how the decsion boundary is with respect to the validation data\n",
    "#decision_b(x_val, d_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3 (#9)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 6-9\n",
    "\n",
    "Now we are going to look at a regression problem. The data as described above (regr1) consists of 6 inputs (features) and one output (target) value. As for previous examples a new data set is generated each time you call the *regr1* function. To get exactly the same data set between different calls, use a fixed seed. New for this problem is that one can also control the amount of noise added to the target signal. We are going to use a relatively small training dataset (~250) and a larger validation dataset (~1000) to get a more robust estimation of the generalization performance. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "The cell below can be used as an template for all questions regarding this regression problem.\n",
    "\n",
    "#### Question 6\n",
    "Use 250 data points for training and about 1000 for validation and **no** added noise. Train an MLP to predict the target output. If you increase the complexity of the model (e.g. number of hidden nodes) you should be able to reach a very small training error. You will also most likely see that the validation error decreases as you increase the complexity or at least no clear sign of overtraining. **Even though the validation error is most likely still larger than the training error why do we not see any overtraining of the model?**\n",
    "\n",
    "**Note:** As with previous examples you may need to tune the optimization parameters to make sure that you have \"optimal\" training. That is, increase or decrease the learningrate, possibly train longer times (increase *epochs*) and change the *batch_size* parameter.\n",
    "\n",
    "#### Question 7\n",
    "Use the same training and validation data sets as above, but add 0.4 units of noise (set the second parameter when calling the *regr1* function to 0.4 for both training and validation). Now train again, starting with a \"small\" model and increase the number of hidden nodes as you monitor the validation result for each model. **How large model do you need in order to see overtraining?** Make a note of the validation error you obtained a this point!\n",
    "\n",
    "#### Question 8\n",
    "Instead of using the number of hidden nodes to control the complexity it is often better to use a regularization term added to the error function. You are now going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). You should modify this value until you find the \"near optimal\" validation performance. The number of hidden nodes should at this point be larger than the number found in the previous question. **Give the L2 value and the number of hidden nodes you found to give \"optimal\" validation performance.**\n",
    "\n",
    "#### Question 9 \n",
    "**Summarize your findings in a few sentences.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training and validation data\n",
    "x_train, d_train = regr1(250, 0.0) # 250 data points with no noise\n",
    "x_val, d_val = regr1(1000, 0.0)\n",
    "\n",
    "# Here we need to normalize the target values\n",
    "norm_m = d_train.mean(axis=0)\n",
    "norm_s = d_train.std(axis=0)\n",
    "d_train = (d_train - norm_m) / norm_s\n",
    "\n",
    "# We use the same normalization for the validation data.\n",
    "d_val = (d_val - norm_m) / norm_s\n",
    "\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [3],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'linear',           # output activation function\n",
    "         'opt_method': 'adam',               # minimization method\n",
    "         'cost_fun': 'mse',                 # error function\n",
    "         'lr_rate': 0.01,                   # learningrate\n",
    "         'lambd' : 0.0}                     # L2 regularization parameter    \n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "#print(model.get_config())\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 1000,      # Number of epochs\n",
    "                      validation_data=(x_val, d_val),\n",
    "                      #batch_size = x_train.shape[0],   # Batch size = all data (batch learning)\n",
    "                      batch_size=50,                    # Batch size for true SGD\n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "pred_trn = model.predict(x_train).reshape(d_train.shape)\n",
    "pred_val = model.predict(x_val).reshape(d_val.shape)\n",
    "stats_reg(d_train, pred_trn, 'Training', estimator)\n",
    "stats_reg(d_val, pred_val, 'Validation', estimator)\n",
    "\n",
    "# Scatter plots of predicted and true values\n",
    "plt.figure()\n",
    "plt.plot(d_train, pred_trn, 'g*', label='Predict vs True (Training)')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(d_val, pred_val, 'b*', label='Predict vs True (Validation)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure()\n",
    "plt.plot(estimator.history['loss'], label='Training')\n",
    "plt.plot(estimator.history['val_loss'], label='Validation')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex4 (#10)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 10\n",
    "\n",
    "For this last exercise you are given a classification problem with a fixed training- and validation dataset. The data is the Pima Indians dataset described in the first cell. Your task is to perform a model section, coming up with your optimal MLP architecture together with the hyperparameters you used. We do not provide any python code for this question, only the part that reads the data from the two external files, *pima_trn.csv* and *pima_val.csv*.\n",
    "\n",
    "#### Question 10\n",
    " **Present an MLP with associated hyperparameters that maximizes the validation performance.**\n",
    "\n",
    "**Hint 1:** \n",
    "For classification problems it is often important to normalize the **input** data. Se the \"Ex3\" cell how this was done (in that case for the target data).\n",
    "\n",
    "**Hint 2:** You are allowed to use the ensemble technique to further increase the performance of the MLP (but no requirement).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pima_t = np.loadtxt(\"pima-trn.csv\", skiprows= 1, \n",
    "                    usecols = tuple(np.arange(1,10)) ).astype(\"float32\")\n",
    "pima_v = np.loadtxt(\"pima-val.csv\", skiprows= 1,\n",
    "                    usecols = tuple(np.arange(1,10)) ).astype(\"float32\")\n",
    "\n",
    "\n",
    "x_train, d_train = pima_t[:,:8], pima_t[:,-1]\n",
    "x_val, d_val = pima_v[:,:8], pima_v[:,-1]\n",
    "\n",
    "print('\\n training data shape: ', x_train.shape, '\\n testing data shape: ',x_val.shape)\n",
    "\n",
    "## Your code below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "\n",
    "### Name\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Answers to questions\n",
    "\n",
    "### Summary\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
